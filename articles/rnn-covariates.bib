@misc{salinas2019deepar,
  title         = {DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks},
  author        = {David Salinas and Valentin Flunkert and Jan Gasthaus},
  year          = {2019},
  eprint        = {1704.04110},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735}
}

@inproceedings{DBLP:conf/nips/GodahewaBWHM21,
  author    = {Rakshitha Godahewa and Christoph Bergmeir and Geoffrey I. Webb and Rob J. Hyndman and Pablo Montero-Manso},
  title     = {Monash Time Series Forecasting Archive},
  year      = {2021},
  cdate     = {1609459200000},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract-round2.html},
  booktitle = {NeurIPS Datasets and Benchmarks},
  crossref  = {conf/nips/2021db}
}

@proceedings{conf/nips/2021db,
  title     = {34th Conference on Neural Information Processing Systems},
  year      = {2021},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/book/2021}
}

@misc{salinas2019highdimensional,
  title         = {High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes},
  author        = {David Salinas and Michael Bohlke-Schneider and Laurent Callot and Roberto Medico and Jan Gasthaus},
  year          = {2019},
  eprint        = {1910.03002},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{NIPS2016_85422afb,
  author    = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/85422afb467e9456013a2a51d4dff702-Paper.pdf},
  volume    = {29},
  year      = {2016}
}

@misc{lai2018modeling,
  title         = {Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks},
  author        = {Guokun Lai and Wei-Cheng Chang and Yiming Yang and Hanxiao Liu},
  year          = {2018},
  eprint        = {1703.07015},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{NEURIPS2018_5cf68969,
  author    = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Deep State Space Models for Time Series Forecasting},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf},
  volume    = {31},
  year      = {2018}
}

@book{hyndman2021forecasting,
  title     = {Forecasting: principles and practice},
  author    = {Hyndman, R.J. and Athanasopoulos, G.},
  year      = {2021},
  edition   = {3},
  publisher = {OTexts},
  address   = {Melbourne, Australia},
  url       = {https://OTexts.com/fpp3},
  note      = {Accessed on 2024-03-21}
}

@article{puindi2020dynamic,
  title={Dynamic structural models with covariates for short-term forecasting of time series with complex seasonal patterns},
  author={Puindi, AC and Silva, ME},
  journal={J Appl Stat},
  volume={48},
  number={5},
  pages={804-826},
  year={2020},
  doi={10.1080/02664763.2020.1748178}
}

@misc{wang2006,
  author = {Wang, Shuchun},
  year = {2006},
  month = {01},
  pages = {},
  title = {Exponential Smoothing for Forecasting and Bayesian Validation of Computer Models}
}


@article{elman1990,
author = {Elman, Jeffrey L.},
title = {Finding Structure in Time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
doi = {https://doi.org/10.1207/s15516709cog1402\_1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
year = {1990}
}

@misc{chen2023tsmixer,
      title={TSMixer: An All-MLP Architecture for Time Series Forecasting}, 
      author={Si-An Chen and Chun-Liang Li and Nate Yoder and Sercan O. Arik and Tomas Pfister},
      year={2023},
      eprint={2303.06053},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wu2022autoformer,
      title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}, 
      author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2106.13008},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2021informer,
      title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}, 
      author={Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
      year={2021},
      eprint={2012.07436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kitaev2020reformer,
      title={Reformer: The Efficient Transformer}, 
      author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
      year={2020},
      eprint={2001.04451},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{gluonts_arxiv,
  author  = {Alexandrov, A. and Benidis, K. and Bohlke-Schneider, M. and
    Flunkert, V. and Gasthaus, J. and Januschowski, T. and Maddix, D. C.
    and Rangapuram, S. and Salinas, D. and Schulz, J. and Stella, L. and
    Türkmen, A. C. and Wang, Y.},
  title   = {{GluonTS: Probabilistic Time Series Modeling in Python}},
  journal = {arXiv preprint arXiv:1906.05264},
  year    = {2019}
}

@misc{smith2018superconvergence,
      title={Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates}, 
      author={Leslie N. Smith and Nicholay Topin},
      year={2018},
      eprint={1708.07120},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hinton2012improving,
      title={Improving neural networks by preventing co-adaptation of feature detectors}, 
      author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
      year={2012},
      eprint={1207.0580},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oreshkin2020nbeats,
      title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting}, 
      author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
      year={2020},
      eprint={1905.10437},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oord2016wavenet,
      title={WaveNet: A Generative Model for Raw Audio}, 
      author={Aaron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
      year={2016},
      eprint={1609.03499},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{zhang2023crossformer,
  title={Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting},
  author={Yunhao Zhang and Junchi Yan},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=vSVLM2j9eie}
}

@misc{zeng2022transformers,
      title={Are Transformers Effective for Time Series Forecasting?}, 
      author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
      year={2022},
      eprint={2205.13504},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{godahewa_2021_4656014,
  author       = {Godahewa, Rakshitha and
                  Bergmeir, Christoph and
                  Webb, Geoff and
                  Hyndman, Rob and
                  Montero-Manso, Pablo},
  title        = {Hospital Dataset},
  month        = {apr},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {2},
  doi          = {10.5281/zenodo.4656014},
  url          = {https://doi.org/10.5281/zenodo.4656014}
}
@misc{godahewa_2021_4656096,
  author       = {Godahewa, Rakshitha and
                  Bergmeir, Christoph and
                  Webb, Geoff and
                  Hyndman, Rob and
                  Montero-Manso, Pablo},
  title        = {Tourism Monthly Dataset},
  month        = {apr},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {3},
  doi          = {10.5281/zenodo.4656096},
  url          = {https://doi.org/10.5281/zenodo.4656096}
}

@misc{godahewa_2021_4656135,
  author       = {Godahewa, Rakshitha and
                  Bergmeir, Christoph and
                  Webb, Geoff and
                  Hyndman, Rob and
                  Montero-Manso, Pablo},
  title        = {Traffic Weekly Dataset},
  month        = {apr},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {3},
  doi          = {10.5281/zenodo.4656135},
  url          = {https://doi.org/10.5281/zenodo.4656135}
}

@misc{godahewa_2021_4656140,
  author       = {Godahewa, Rakshitha and
                  Bergmeir, Christoph and
                  Webb, Geoff and
                  Hyndman, Rob and
                  Montero-Manso, Pablo},
  title        = {Electricity Hourly Dataset},
  month        = {apr},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {3},
  doi          = {10.5281/zenodo.4656140},
  url          = {https://doi.org/10.5281/zenodo.4656140}
}